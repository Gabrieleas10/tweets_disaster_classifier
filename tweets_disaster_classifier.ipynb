{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "d7ebc6ada24cc9534b2d592d0970f4826ac2b0fb65472f81fe2df442270f7538"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re ,string\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.metrics import *\n",
    "import emoji\n",
    "import regex\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import spacy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\GABRIEL\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\GABRIEL\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\GABRIEL\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     C:\\Users\\GABRIEL\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\GABRIEL\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# installing nltk packages\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "\n",
    "train_dataset = 'E:\\\\Projects\\\\tweets_disaster_classifier\\\\train.csv'\n",
    "test_dataset = 'E:\\\\Projects\\\\tweets_disaster_classifier\\\\test.csv'\n",
    "submit = 'E:\\\\Projects\\\\tweets_disaster_classifier\\\\sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "df_train = pd.read_csv(train_dataset , sep =',')\n",
    "df_test = pd.read_csv(test_dataset , sep =',')\n",
    "df_sample = pd.read_csv(submit , sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 id      target\n",
       "count   7613.000000  7613.00000\n",
       "mean    5441.934848     0.42966\n",
       "std     3137.116090     0.49506\n",
       "min        1.000000     0.00000\n",
       "25%     2734.000000     0.00000\n",
       "50%     5408.000000     0.00000\n",
       "75%     8146.000000     1.00000\n",
       "max    10873.000000     1.00000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7613.000000</td>\n      <td>7613.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5441.934848</td>\n      <td>0.42966</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3137.116090</td>\n      <td>0.49506</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2734.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5408.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8146.000000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10873.000000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "deluge                   42\n",
       "armageddon               42\n",
       "harm                     41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_train['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "USA                               104\n",
       "New York                           71\n",
       "United States                      50\n",
       "London                             45\n",
       "Canada                             29\n",
       "                                 ... \n",
       "Uruguay / Westeros / Gallifrey      1\n",
       "Alameda and Pleasanton, CA          1\n",
       "USA/SO FLORIDA via BROOKLYN NY      1\n",
       "The Sanctuary Network, Rome         1\n",
       "Nicoma Park, OK                     1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_train['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 102.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values by '0'\n",
    "\n",
    "df_train['keyword'] = df_train['keyword'].replace(np.nan, '0')\n",
    "df_train['location'] = df_train['location'].replace(np.nan, '0')\n",
    "\n",
    "df_test['keyword'] = df_test['keyword'].replace(np.nan, '0')\n",
    "df_test['location'] = df_test['location'].replace(np.nan, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Target Count')"
      ]
     },
     "metadata": {},
     "execution_count": 19
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"280.35375pt\" version=\"1.1\" viewBox=\"0 0 395.328125 280.35375\" width=\"395.328125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-03-07T21:08:13.992758</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 280.35375 \r\nL 395.328125 280.35375 \r\nL 395.328125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 53.328125 242.7975 \r\nL 388.128125 242.7975 \r\nL 388.128125 25.3575 \r\nL 53.328125 25.3575 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p28530f3797)\" d=\"M 70.068125 242.7975 \r\nL 203.988125 242.7975 \r\nL 203.988125 35.711786 \r\nL 70.068125 35.711786 \r\nz\r\n\" style=\"fill:#3274a1;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p28530f3797)\" d=\"M 237.468125 242.7975 \r\nL 371.388125 242.7975 \r\nL 371.388125 86.791657 \r\nL 237.468125 86.791657 \r\nz\r\n\" style=\"fill:#e1812c;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1940c18909\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"137.028125\" xlink:href=\"#m1940c18909\" y=\"242.7975\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(133.846875 257.395938)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.428125\" xlink:href=\"#m1940c18909\" y=\"242.7975\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(301.246875 257.395938)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_3\">\r\n     <!-- target -->\r\n     <g transform=\"translate(205.525 271.074062)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"139.851562\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"203.328125\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"264.851562\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_3\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4a5d883a0d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m4a5d883a0d\" y=\"242.7975\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(39.965625 246.596719)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m4a5d883a0d\" y=\"195.103876\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(20.878125 198.903095)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m4a5d883a0d\" y=\"147.410253\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2000 -->\r\n      <g transform=\"translate(20.878125 151.209471)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m4a5d883a0d\" y=\"99.716629\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3000 -->\r\n      <g transform=\"translate(20.878125 103.515848)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.328125\" xlink:href=\"#m4a5d883a0d\" y=\"52.023005\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 4000 -->\r\n      <g transform=\"translate(20.878125 55.822224)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_9\">\r\n     <!-- count -->\r\n     <g transform=\"translate(14.798438 148.18375)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"116.162109\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"179.541016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"242.919922\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 53.328125 242.7975 \r\nL 53.328125 25.3575 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 388.128125 242.7975 \r\nL 388.128125 25.3575 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 53.328125 242.7975 \r\nL 388.128125 242.7975 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 53.328125 25.3575 \r\nL 388.128125 25.3575 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_10\">\r\n    <!-- Target Count -->\r\n    <g transform=\"translate(169.673125 19.3575)scale(0.16 -0.16)\">\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"44.583984\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"105.863281\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"145.226562\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"208.703125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"270.226562\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"309.435547\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"341.222656\" xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"411.046875\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"472.228516\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"535.607422\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"598.986328\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p28530f3797\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"53.328125\" y=\"25.3575\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEYCAYAAACtEtpmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUpklEQVR4nO3df7RdZZ3f8feHgPyoUmASkEmoYdmUKdARS4qg1bHqqpmihupgY3UIM0wzBeYH7dRZ0NVVnbG0trVacAa60o4mjFYaHR2QGWqZVPxVfngRlV+TRSoIKZEEGDQ4TmaC3/6xd4bj5eQ+h5Bzz03u+7XWXufsZz97n++56+Z8sp9nn31TVUiSNJODJl2AJGnuMywkSU2GhSSpybCQJDUZFpKkJsNCktRkWGi/kKRGWB6cdJ2DklyS5K3PcZ+Tk3w0ybeT7Ezy3SRfSvIrSQ4bV60j1nZUkvcm+duTrEOTcfCkC5BGdNa09c8A3wDeO9C2c9aqGc0lwJeBT4/SOcm5wO8CdwPvA+4H/grwU8BvAAGuGEehIzoKeA+wBfjaBOvQBBgW2i9U1a2D60l2Ao9Nb98bSQ6tqokGTZJlwDXAjcC5VbVrYPMfJvkA8DcmUpyEw1A6QCQ5LMmHktyd5Kkk30ny2SQ/Ma3f+f2Q1WuSfDLJk8Bt/bYjklyd5PEkO5J8Jskr+/7nTzvOTyXZ2Pf7fpLPJTl1YPuDwEuAdw4Mk62b4S38M7r/vF00LSgAqKrtVfWVgeOf1Nf3ZJIfJLk1yYppNa4bNjSX5OYkNw+sv7av7y1JfivJY0m2J/lYkqP6PkuBB/pd/uvAezp/+vF1YDIsdKA4FHgR8G+As4ELgcOAW5O8eEj/j9N9+P0McGnfthb4eeADwFuBTX2/H5HkbGAj8BTwLuAf96/9pSQn9N3+IfAd4HN0Q2hn0Q0t7ckbgK9W1dbWG03y43TDWy8Dfgl4O/Ak8AdJfrq1/wyuAIru/fwm8DaeGfbaSvczAfh3PPOe/uB5vJ72Iw5D6YBQVd8FfmH3epIFdB/UjwLvAD40bZdPVdWvD/Q/ie5D8tKq+g99801JjgB+edq+VwBfqKqVA/t/HvgW8GvAJVV153McKjsBuGOEfgD/HDgaOKuqNvev/4fAvcDldENZe+OLVbX7vf6v/mfyC0nOr6qdSe7st31rXwz/af/imYUOGEnenuS2fmhpF/B94IXASUO6f2ba+ivoJpA/Oa39U9NeYxnwUuDjSQ7evQB/CtwCvOZ5v5G21wC37g4KgKp6GvgEcFqSI/fyuNPPEu6iO2M7bi+PpwOIYaEDQpI3A/8DuI/uDOEVwN8BttMNR003fbjn+P5x27T2R6etH9s//g7wF9OWNwE/thflAzxMN8cximN4dv3QDXuF7qxjbzwxbX33pP9EL9nV3OAwlA4Uq4DNVXX+7oYkh9B9sA4z/d78uz98j+WZiVx49v+qH+8fLwP+aMhx/3yUYof4I7ohnxdX1XcafZ8Ahs3DvJjufe3+0P8z4AVD+v0Yz7wPaSSeWehAcQTd0NOgnwUWjLj/bXQftOdOa5++vgl4EDilqqaGLN8c6LsTOHzE1/8Q8DRwVT/f8iOSLEzyqn71C8CZ/RVKu7cvAP4RcGdV7eibvw0cl2ThQL+XMnxYbhS7zzRGfU86gHhmoQPF/wTOSfIh4AbgdOBX6K4SaqqqTUn+O/C+JAfRTTa/Dnhz3+WHfb9KcjFwXZIXABuAx+jOQF4JPFRVH+z3uRd4dZI30Q0RPVZVD+7h9e9Pch7wMboruP4Lz3wp79XAL9JdofQVumA5n24C/j3A94CL6L6HcfbAYT9JdwXWx5N8EFhId0b02Cg/kyEepTsjWZXkm3RzQg9UlWcp80FVubjsdwvd/+4/NrB+EN1ls4/QTTZ/AXh532/dQL/z6c4g/vqQYx4BXE03jPMUcD3dh28BK6f1PYsulP6EbrjnQeBauiuUdvf5CeBLfT01WMcM7+sUYB3wEN2Q1nf7Y1wEHDrQ7yTg9/vtfwbcCqwYcrxz6L4R/gO6b7z/feBm4OaBPq/t63vDtH13/6yWTjvevXRzNAWcP+nfBZfZWdL/AkgaIsm7gX9P94H50KTrkSbFYSip1w8XnQp8nW7Y6dXAvwA2GBSa7wwL6Rk76IZZLqWbK/h/wJV0N8+T5jWHoSRJTV46K0lqOmCHoRYuXFhLly6ddBmStF+54447HquqRdPbD9iwWLp0KVNTU5MuQ5L2K0m+PazdYShJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVLTAfsN7ufr9HdfM+kSNAfd8R/Pm3QJ0kR4ZiFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNY09LJIsSHJnkhv69WOS3JTk/v7x6IG+lyXZnGRTkjcOtJ+e5K5+25VJMu66JUnPmI0zi18F7htYvxTYWFXLgI39OklOBlYBpwArgKuSLOj3uRpYAyzrlxWzULckqTfWsEiyBDgb+G8DzSuB9f3z9cA5A+3XVtXOqnoA2AyckeR44MiquqWqCrhmYB9J0iwY95nFfwZ+HfjhQNtxVbUVoH88tm9fDDw80G9L37a4fz69/VmSrEkylWRq+/bt++QNSJLGGBZJ3gRsq6o7Rt1lSFvN0P7sxqq1VbW8qpYvWrRoxJeVJLWM8y/lvQp4S5J/ABwGHJnkY8CjSY6vqq39ENO2vv8W4ISB/ZcAj/TtS4a0S5JmydjOLKrqsqpaUlVL6Sau/3dVvQu4Hljdd1sNXNc/vx5YleTQJCfSTWTf3g9V7UhyZn8V1HkD+0iSZsEk/gb3+4ENSS4AHgLOBaiqe5JsAO4FdgEXV9XT/T4XAuuAw4Eb+0WSNEtmJSyq6mbg5v7548Dr99DvcuDyIe1TwKnjq1CSNBO/wS1JajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpKaDJ12ApOfuod/8W5MuQXPQX/vXd43t2J5ZSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmsYWFkkOS3J7km8kuSfJb/TtxyS5Kcn9/ePRA/tclmRzkk1J3jjQfnqSu/ptVybJuOqWJD3bOM8sdgKvq6qXAacBK5KcCVwKbKyqZcDGfp0kJwOrgFOAFcBVSRb0x7oaWAMs65cVY6xbkjTN2MKiOk/1q4f0SwErgfV9+3rgnP75SuDaqtpZVQ8Am4EzkhwPHFlVt1RVAdcM7CNJmgVjnbNIsiDJ14FtwE1VdRtwXFVtBegfj+27LwYeHth9S9+2uH8+vX3Y661JMpVkavv27fv0vUjSfDbWsKiqp6vqNGAJ3VnCqTN0HzYPUTO0D3u9tVW1vKqWL1q06DnXK0kablauhqqqJ4Gb6eYaHu2Hlugft/XdtgAnDOy2BHikb18ypF2SNEvGeTXUoiRH9c8PB94A/DFwPbC677YauK5/fj2wKsmhSU6km8i+vR+q2pHkzP4qqPMG9pEkzYJx/vGj44H1/RVNBwEbquqGJLcAG5JcADwEnAtQVfck2QDcC+wCLq6qp/tjXQisAw4HbuwXSdIsGVtYVNU3gZcPaX8ceP0e9rkcuHxI+xQw03yHJGmM/Aa3JKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqWmksEiycZQ2SdKBacY/q5rkMOAIYGGSo4H0m44EfnzMtUmS5ojW3+D+ReASumC4g2fC4nvAb4+vLEnSXDJjWFTVFcAVSX65qj48SzVJkuaY1pkFAFX14SSvBJYO7lNV14ypLknSHDJSWCT5XeClwNeBp/vmAgwLSZoHRgoLYDlwclXVOIuRJM1No37P4m7gxeMsRJI0d416ZrEQuDfJ7cDO3Y1V9ZaxVCVJmlNGDYv3jrMISdLcNurVUF8YdyGSpLlr1KuhdtBd/QTwAuAQ4PtVdeS4CpMkzR2jnlm8aHA9yTnAGeMoSJI09+zVXWer6veB1+3bUiRJc9Wow1BvHVg9iO57F37nQpLmiVGvhnrzwPNdwIPAyn1ejSRpThp1zuLnxl2IJGnuGvWPHy1J8pkk25I8muT3kiwZd3GSpLlh1AnujwLX0/1di8XAZ/s2SdI8MGpYLKqqj1bVrn5ZBywaY12SpDlk1LB4LMm7kizol3cBj4+zMEnS3DFqWPw88HbgO8BW4GeAGSe9k5yQ5PNJ7ktyT5Jf7duPSXJTkvv7x6MH9rksyeYkm5K8caD99CR39duuTJJhrylJGo9Rw+J9wOqqWlRVx9KFx3sb++wCfq2q/iZwJnBxkpOBS4GNVbUM2Niv029bBZwCrACuSrKgP9bVwBpgWb+sGLFuSdI+MGpY/GRV/cnulap6Anj5TDtU1daq+lr/fAdwH93k+Epgfd9tPXBO/3wlcG1V7ayqB4DNwBlJjgeOrKpb+j++dM3APpKkWTBqWBw0bbjoGEb/Qh9JltKFy23AcVW1FbpAAY7tuy0GHh7YbUvftrh/Pr192OusSTKVZGr79u2jlidJahj1A/8/Af8nyafobvPxduDyUXZM8kLg94BLqup7M0w3DNtQM7Q/u7FqLbAWYPny5d6ORJL2kVG/wX1Nkim6mwcGeGtV3dvaL8khdEHx8ar6dN/8aJLjq2prP8S0rW/fApwwsPsS4JG+fcmQdknSLBn5rrNVdW9V/VZVfXjEoAjwO8B9VfXBgU3XA6v756uB6wbaVyU5NMmJdBPZt/dDVTuSnNkf87yBfSRJs2DkeYe98CrgZ4G7kny9b/uXwPuBDUkuAB4CzgWoqnuSbADupbuS6uKqerrf70JgHXA4cGO/SJJmydjCoqq+zPD5BoDX72GfyxkyF1JVU8Cp+646SdJzsVd//EiSNL8YFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTWMLiyQfSbItyd0DbcckuSnJ/f3j0QPbLkuyOcmmJG8caD89yV39tiuTZFw1S5KGG+eZxTpgxbS2S4GNVbUM2Nivk+RkYBVwSr/PVUkW9PtcDawBlvXL9GNKksZsbGFRVV8EnpjWvBJY3z9fD5wz0H5tVe2sqgeAzcAZSY4HjqyqW6qqgGsG9pEkzZLZnrM4rqq2AvSPx/bti4GHB/pt6dsW98+ntw+VZE2SqSRT27dv36eFS9J8NlcmuIfNQ9QM7UNV1dqqWl5VyxctWrTPipOk+W62w+LRfmiJ/nFb374FOGGg3xLgkb59yZB2SdIsmu2wuB5Y3T9fDVw30L4qyaFJTqSbyL69H6rakeTM/iqo8wb2kSTNkoPHdeAknwBeCyxMsgV4D/B+YEOSC4CHgHMBquqeJBuAe4FdwMVV9XR/qAvprqw6HLixXyRJs2hsYVFV79jDptfvof/lwOVD2qeAU/dhaZKk52iuTHBLkuYww0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqcmwkCQ1GRaSpCbDQpLUZFhIkpoMC0lSk2EhSWoyLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSJKaDAtJUpNhIUlqMiwkSU2GhSSpybCQJDUZFpKkJsNCktRkWEiSmgwLSVKTYSFJajIsJElNhoUkqWm/CYskK5JsSrI5yaWTrkeS5pP9IiySLAB+G/hp4GTgHUlOnmxVkjR/7BdhAZwBbK6qb1XVnwPXAisnXJMkzRsHT7qAES0GHh5Y3wK8YnqnJGuANf3qU0k2zUJt88FC4LFJFzEX5AOrJ12Cns3fz93ek31xlJcMa9xfwmLYT6Ce1VC1Flg7/nLmlyRTVbV80nVIw/j7OTv2l2GoLcAJA+tLgEcmVIskzTv7S1h8FViW5MQkLwBWAddPuCZJmjf2i2GoqtqV5JeAzwELgI9U1T0TLms+cWhPc5m/n7MgVc8a+pck6UfsL8NQkqQJMiwkSU2GhWbkbVY0VyX5SJJtSe6edC3zgWGhPfI2K5rj1gErJl3EfGFYaCbeZkVzVlV9EXhi0nXMF4aFZjLsNiuLJ1SLpAkyLDSTkW6zIunAZ1hoJt5mRRJgWGhm3mZFEmBYaAZVtQvYfZuV+4AN3mZFc0WSTwC3ACcl2ZLkgknXdCDzdh+SpCbPLCRJTYaFJKnJsJAkNRkWkqQmw0KS1GRYSHshyVFJLpqF1znHmzdqLjAspL1zFDByWKSzN//ezqG74680UX7PQtoLSXbfgXcT8HngJ4GjgUOAf1VV1yVZCtzYbz+L7oP/POCddDdofAy4o6o+kOSldLeDXwT8KfBPgGOAG4Dv9svbqur/ztJblH7EwZMuQNpPXQqcWlWnJTkYOKKqvpdkIXBrkt23RTkJ+LmquijJcuBtwMvp/u19Dbij77cW+KdVdX+SVwBXVdXr+uPcUFWfms03J01nWEjPX4B/m+Q1wA/pbuN+XL/t21V1a//87wLXVdUPAJJ8tn98IfBK4JPJX97o99BZql0aiWEhPX/vpBs+Or2q/iLJg8Bh/bbvD/Qbdst36OYOn6yq08ZWofQ8OcEt7Z0dwIv6538V2NYHxd8DXrKHfb4MvDnJYf3ZxNkAVfU94IEk58JfToa/bMjrSBNjWEh7oaoeB76S5G7gNGB5kim6s4w/3sM+X6W7xfs3gE8DU3QT1/T7XZDkG8A9PPPna68F3p3kzn4SXJoIr4aSZlGSF1bVU0mOAL4IrKmqr026LqnFOQtpdq3tv2R3GLDeoND+wjMLSVKTcxaSpCbDQpLUZFhIkpoMC0lSk2EhSWr6/5pUMQJoBmLuAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# plotting value counts of Rating (balaced dataset)\n",
    "\n",
    "sns.countplot(x='target', data= df_train)\n",
    "plt.title('Target Count', size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasforming text in new informations\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x: x.lower())\n",
    "df_train['text_s'] = df_train['text'].apply(lambda x: x.split())\n",
    "df_train['text_s'] = df_train['text_s'].apply(lambda x:[item.replace(',','') for item in x if item.count(',')!=-1])\n",
    "df_train['words_number'] = df_train['text_s'].apply(lambda x: len(x))\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(lambda x: x.lower())\n",
    "df_test['text_s'] = df_test['text'].apply(lambda x: x.split())\n",
    "df_test['text_s'] = df_test['text_s'].apply(lambda x:[item.replace(',','') for item in x if item.count(',')!=-1])\n",
    "df_test['words_number'] = df_test['text_s'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "diswords = ['dead','rain','hurricane','tornado',\n",
    "             'deaths','emergency','fire','wind',\n",
    "             'water','sea','break','crash',\n",
    "            'street','police','murder','ambulance',\n",
    "             'flame','shoot','kill','smoke','killed',\n",
    "            'smoking','broke','breaking','flames',\n",
    "            'murderer','polices','death','flood',\n",
    "            'flooding','rainfall','rescue','train',\n",
    "            'degrees','disaster','suicide','bomb',\n",
    "            'bomber','wildfire','fatal','risk',\n",
    "            'disasters','storm','shooting','accident',\n",
    "            'explosion','cyclone','accidents','collision',\n",
    "            'landslides','earthquake','lightening',\n",
    "            'avalanche','cold','snow','trains','hospital',\n",
    "            'gas','floods','outbreak','hurricane',\n",
    "            'landslide','flash','shipwreck','typhoon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_words(x):\n",
    "    a = 0\n",
    "    for i in range(0, len(diswords),1):\n",
    "        if x.find(diswords[i]) != -1:\n",
    "            a = a+1\n",
    "        else:\n",
    "            pass\n",
    "    return a   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['dis_words'] = df_train['text'].apply(dis_words)\n",
    "df_test['dis_words'] = df_test['text'].apply(dis_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy to Named Entity Recognition\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df_train['text_nlp'] = df_train['text'].apply(lambda x: nlp(x))\n",
    "df_train['nlp_types'] = df_train['text_nlp'].apply(lambda x: [item.ent_type_ for item in x if item.ent_type_] )\n",
    "df_train['mf_class'] = df_train['nlp_types'].apply(lambda x: max(set(x) , key = x.count) if len(x)!=0 else '0')\n",
    "df_train['org_class'] = df_train['nlp_types'].apply(lambda x: x.count('ORG'))\n",
    "df_train['gpe_class'] = df_train['nlp_types'].apply(lambda x: x.count('GPE'))\n",
    "df_train['car_class'] = df_train['nlp_types'].apply(lambda x: x.count('CARDINAL'))\n",
    "df_train['dat_class'] = df_train['nlp_types'].apply(lambda x: x.count('DATE'))\n",
    "df_train['per_class'] = df_train['nlp_types'].apply(lambda x: x.count('PERSON'))\n",
    "df_train['mon_class'] = df_train['nlp_types'].apply(lambda x: x.count('MONEY'))\n",
    "df_train['loc_class'] = df_train['nlp_types'].apply(lambda x: x.count('LOC'))\n",
    "df_train['tim_class'] = df_train['nlp_types'].apply(lambda x: x.count('TIME'))\n",
    "df_train['pro_class'] = df_train['nlp_types'].apply(lambda x: x.count('PRODUCT'))\n",
    "df_train['nor_class'] = df_train['nlp_types'].apply(lambda x: x.count('NORP'))\n",
    "df_train['qtd_class'] = df_train['nlp_types'].apply(lambda x: x.count('QUANTITY'))\n",
    "df_train['fac_class'] = df_train['nlp_types'].apply(lambda x: x.count('FAC'))\n",
    "df_train['nlp_types_syn'] = df_train['text_nlp'].apply(lambda x: [item.dep_ for item in x if item.dep_] )\n",
    "df_train['sp_class'] = df_train['nlp_types_syn'].apply(lambda x: max(set(x) , key = x.count) if len(x)!=0 else '0')\n",
    "df_train['pun_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('punct'))\n",
    "df_train['com_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('compound'))\n",
    "df_train['rot_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('ROOT'))\n",
    "df_train['amd_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('amod'))\n",
    "df_train['pre_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('prep'))\n",
    "df_train['nsu_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('nsubj'))\n",
    "df_train['dob_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('dobj'))\n",
    "df_train['det_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('det'))\n",
    "df_train['adv_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('advmod'))\n",
    "df_train['pob_class'] = df_train['nlp_types_syn'].apply(lambda x: x.count('pobj'))\n",
    "\n",
    "# test dataset\n",
    "\n",
    "df_test['text_nlp'] = df_test['text'].apply(lambda x: nlp(x))\n",
    "df_test['nlp_types'] = df_test['text_nlp'].apply(lambda x: [item.ent_type_ for item in x if item.ent_type_] )\n",
    "df_test['mf_class']  = df_test['nlp_types'].apply(lambda x: max(set(x) , key = x.count) if len(x)!=0 else '0')\n",
    "df_test['org_class'] = df_test['nlp_types'].apply(lambda x: x.count('ORG'))\n",
    "df_test['gpe_class'] = df_test['nlp_types'].apply(lambda x: x.count('GPE'))\n",
    "df_test['car_class'] = df_test['nlp_types'].apply(lambda x: x.count('CARDINAL'))\n",
    "df_test['dat_class'] = df_test['nlp_types'].apply(lambda x: x.count('DATE'))\n",
    "df_test['per_class'] = df_test['nlp_types'].apply(lambda x: x.count('PERSON'))\n",
    "df_test['mon_class'] = df_test['nlp_types'].apply(lambda x: x.count('MONEY'))\n",
    "df_test['loc_class'] = df_test['nlp_types'].apply(lambda x: x.count('LOC'))\n",
    "df_test['tim_class'] = df_test['nlp_types'].apply(lambda x: x.count('TIME'))\n",
    "df_test['pro_class'] = df_test['nlp_types'].apply(lambda x: x.count('PRODUCT'))\n",
    "df_test['nor_class'] = df_test['nlp_types'].apply(lambda x: x.count('NORP'))\n",
    "df_test['qtd_class'] = df_test['nlp_types'].apply(lambda x: x.count('QUANTITY'))\n",
    "df_test['fac_class'] = df_test['nlp_types'].apply(lambda x: x.count('FAC'))\n",
    "df_test['nlp_types_syn'] = df_test['text_nlp'].apply(lambda x: [item.dep_ for item in x if item.dep_] )\n",
    "df_test['sp_class'] = df_test['nlp_types_syn'].apply(lambda x: max(set(x) , key = x.count) if len(x)!=0 else '0')\n",
    "df_test['pun_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('punct'))\n",
    "df_test['com_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('compound'))\n",
    "df_test['rot_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('ROOT'))\n",
    "df_test['amd_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('amod'))\n",
    "df_test['pre_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('prep'))\n",
    "df_test['nsu_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('nsubj'))\n",
    "df_test['dob_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('dobj'))\n",
    "df_test['det_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('det'))\n",
    "df_test['adv_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('advmod'))\n",
    "df_test['pob_class'] = df_test['nlp_types_syn'].apply(lambda x: x.count('pobj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                  forest fire near la ronge sask. canada\n",
       "2       all residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @thetawniest the out of control w...\n",
       "7610    m1.94 [01:04 utc]?5km s of volcano hawaii. htt...\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest: more homes razed by northern calif...\n",
       "Name: text, Length: 3271, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_train.loc[df_train['target'] == 1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing list of words: positive and negative\n",
    "\n",
    "eas = os.path.join('E:\\\\Projects\\\\tweets_disaster_classifier\\\\words_list')\n",
    "pos_words_file = os.path.join(eas, 'positive_words.txt')\n",
    "neg_words_file = os.path.join(eas, 'negative_words.txt')\n",
    "\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "# reading .txt files\n",
    "for pos_word in open(pos_words_file, 'r').readlines():\n",
    "    pos_words.append(pos_word.replace('\\n',''))\n",
    "\n",
    "for neg_word in open(neg_words_file, 'r').readlines():\n",
    "    neg_words.append(neg_word.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting positive words in text\n",
    "\n",
    "df_train['positive'] = df_train['text_s'].apply(lambda x: len((set(x) & set(pos_words))))\n",
    "df_test['positive'] = df_test['text_s'].apply(lambda x: len((set(x) & set(pos_words))))\n",
    "\n",
    "# counting negative words in text\n",
    "\n",
    "df_train['negative'] = df_train['text_s'].apply(lambda x: len((set(x) & set(neg_words))))\n",
    "df_test['negative'] = df_test['text_s'].apply(lambda x: len((set(x) & set(neg_words))))\n",
    "\n",
    "# value of neutral words\n",
    "\n",
    "df_train['neutro'] = df_train['words_number'] - (df_train['positive'] + df_train['negative'])\n",
    "df_test['neutro'] = df_test['words_number'] - (df_test['positive'] + df_test['negative'])\n",
    "\n",
    "# percentage of positive words of tweet\n",
    "\n",
    "df_train['positive%'] = df_train['positive'] / df_train['words_number']\n",
    "df_test['positive%'] = df_test['positive'] / df_test['words_number']\n",
    "\n",
    "# percentage of negative words of tweet\n",
    "\n",
    "df_train['negative%'] = df_train['negative'] / df_train['words_number']\n",
    "df_test['negative%'] = df_test['negative'] / df_test['words_number']\n",
    "\n",
    "# difference between positive and negative words of tweet\n",
    "\n",
    "df_train['delta'] = df_train['positive'] - df_train['negative']\n",
    "df_test['delta'] = df_test['positive'] - df_test['negative']\n",
    "\n",
    "# counting first positives words of tweet (0 to 3° word)\n",
    "\n",
    "df_train['first_words_p'] = df_train['text_s'].apply(lambda x: len((set(x[0:3]) & set(pos_words))))\n",
    "df_test['first_words_p'] = df_test['text_s'].apply(lambda x: len((set(x[0:3]) & set(pos_words))))\n",
    "\n",
    "# counting first negatives words of tweet (0 to 3° word)\n",
    "\n",
    "df_train['first_words_n'] = df_train['text_s'].apply(lambda x: len((set(x[0:3]) & set(neg_words))))\n",
    "df_test['first_words_n'] = df_test['text_s'].apply(lambda x: len((set(x[0:3]) & set(neg_words))))\n",
    "\n",
    "# using nltk pack to identify the gramatical class of word\n",
    "\n",
    "df_train['tags'] = df_train['text_s'].apply(lambda x: pos_tag(x))\n",
    "df_test['tags'] = df_test['text_s'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and count emojis\n",
    "\n",
    "df_train['emoji'] = df_train['text'].apply(lambda x:re.findall(r'(:[^:]*:)', emoji.demojize(x)))\n",
    "df_train['emoji_n'] = df_train['emoji'].apply(lambda x: len(x))\n",
    "\n",
    "df_test['emoji'] = df_test['text'].apply(lambda x:re.findall(r'(:[^:]*:)', emoji.demojize(x)))\n",
    "df_test['emoji_n'] = df_test['emoji'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting and count smiles: =) and sad: =(\n",
    "\n",
    "df_train['smile']  = df_train['text'].apply(lambda x: 0 if (x.find('=)') and x.find(';)'))== -1 else 1 )\n",
    "df_train['sad']  = df_train['text'].apply(lambda x: 0 if x.find('=(')!= -1 else 1)\n",
    "\n",
    "df_test['smile']  = df_test['text'].apply(lambda x: 0 if (x.find('=)') and x.find(';)')) == -1 else 1 )\n",
    "df_test['sad']  = df_test['text'].apply(lambda x: 0 if x.find('=(')!= -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting values of gramatical class\n",
    "\n",
    "def tag_type(tag):\n",
    "    tag_list = []\n",
    "    for i in range(0,len(tag),1):\n",
    "        tagt = tag[i][1]\n",
    "        tag_list.append(tagt[0:2])\n",
    "    return tag_list\n",
    "\n",
    "df_train['tag_type'] = df_train['tags'].apply(tag_type)\n",
    "df_test['tag_type'] = df_test['tags'].apply(tag_type)\n",
    "\n",
    "# creating a dict count words of each gramatical class\n",
    "\n",
    "df_train['dict_tag'] = df_train['tag_type'].apply(lambda x :{y:x.count(y) for y in x})\n",
    "df_test['dict_tag'] = df_test['tag_type'].apply(lambda x :{y:x.count(y) for y in x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building function to split gramatical classes in columns\n",
    "\n",
    "def class_type(tag):\n",
    "    try:\n",
    "        adj = tag['JJ']\n",
    "    except:\n",
    "        adj = 0\n",
    "    try:\n",
    "        vb = tag['VB']\n",
    "    except:\n",
    "        vb = 0\n",
    "    try:\n",
    "        subs = tag['NN']\n",
    "    except:\n",
    "        subs = 0\n",
    "    try:\n",
    "        inj = tab['UH']\n",
    "    except:\n",
    "        inj = 0\n",
    "    return pd.Series([adj , vb , subs , inj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting in columns\n",
    "\n",
    "df_train[['adj','vb','subs','inj']] = df_train['dict_tag'].apply(class_type)\n",
    "df_test[['adj','vb','subs','inj']] = df_test['dict_tag'].apply(class_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples tweets from nltk package\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweet_tokens_p = twitter_samples.tokenized('positive_tweets.json')\n",
    "tweet_tokens_n = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building functions to morphological analize and remove noise\n",
    "# 1\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "#2\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the functions\n",
    "\n",
    "tweet_tokens_p = list(map(lambda x: lemmatize_sentence(x), tweet_tokens_p))\n",
    "tweet_tokens_n = list(map(lambda x: lemmatize_sentence(x), tweet_tokens_n))\n",
    "\n",
    "tweet_tokens_p = list(map(lambda x: remove_noise(x), tweet_tokens_p))\n",
    "tweet_tokens_n = list(map(lambda x: remove_noise(x), tweet_tokens_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending all the tweets\n",
    "\n",
    "for i in range(1 , len(tweet_tokens_p),1):\n",
    "    positive_tweets = tweet_tokens_p[0]\n",
    "    positive_tweets.extend(tweet_tokens_p[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1 , len(tweet_tokens_n),1):\n",
    "    negative_tweets = tweet_tokens_n[0]\n",
    "    negative_tweets.extend(tweet_tokens_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting positive words in text\n",
    "\n",
    "df_train['pos_tweet'] = df_train['text_s'].apply(lambda x: len((set(x) & set(positive_tweets))))\n",
    "df_test['pos_tweet'] = df_test['text_s'].apply(lambda x: len((set(x) & set(positive_tweets))))\n",
    "\n",
    "# counting negative words in text\n",
    "\n",
    "df_train['neg_tweet'] = df_train['text_s'].apply(lambda x: len((set(x) & set(negative_tweets))))\n",
    "df_test['neg_tweet'] = df_test['text_s'].apply(lambda x: len((set(x) & set(negative_tweets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between positive and negaative tweets\n",
    "\n",
    "df_train['delta2'] = df_train['pos_tweet'] - df_train['neg_tweet']\n",
    "df_test['delta2'] = df_test['pos_tweet'] - df_test['neg_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to \n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(tweet_tokens_p)\n",
    "all_neg_words = get_all_words(tweet_tokens_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(':)', 7381), ('you', 2926), ('be', 2379), ('to', 2186), ('i', 2186), ('the', 2172), ('a', 1868), ('for', 1541), ('and', 1412), (':-)', 1402)]\n"
     ]
    }
   ],
   "source": [
    "# verifing the frequency of words\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(tweet_tokens_p)\n",
    "negative_tokens_for_model = get_tweets_for_model(tweet_tokens_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encondig keyword\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train['keyword'])\n",
    "df_train['keyword_le'] = le.transform(df_train['keyword'])\n",
    "\n",
    "le.fit(df_test['keyword'])\n",
    "df_test['keyword_le'] = le.transform(df_test['keyword'])\n",
    "\n",
    "le.fit(df_train['mf_class'])\n",
    "df_train['mf_class_le'] = le.transform(df_train['mf_class'])\n",
    "\n",
    "le.fit(df_test['mf_class'])\n",
    "df_test['mf_class_le'] = le.transform(df_test['mf_class'])\n",
    "\n",
    "le.fit(df_train['sp_class'])\n",
    "df_train['sp_class_le'] = le.transform(df_train['sp_class'])\n",
    "\n",
    "le.fit(df_test['sp_class'])\n",
    "df_test['sp_class_le'] = le.transform(df_test['sp_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encondig location\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train['location'])\n",
    "df_train['location_le'] = le.transform(df_train['location'])\n",
    "\n",
    "le.fit(df_test['location'])\n",
    "df_test['location_le'] = le.transform(df_test['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target           1.000000\ndis_words        0.363158\nPCA2             0.212402\npob_class        0.188962\npre_class        0.182836\nsubs             0.177987\nmf_class_le      0.173360\nemoji_n          0.168089\ncom_class        0.168062\ngpe_class        0.125694\namd_class        0.124252\ncar_class        0.118271\nnegative         0.084004\nnegative%        0.073709\nfirst_words_n    0.072950\nadj              0.070448\nqtd_class        0.067198\norg_class        0.064752\nid               0.060789\nnor_class        0.050498\ndat_class        0.048408\nkeyword_le       0.047864\nneutro           0.044140\nwords_number     0.041779\ntim_class        0.040029\nmon_class        0.030994\nper_class        0.026778\nsp_class_le      0.021905\nloc_class        0.019182\npun_class        0.015417\nPCA4             0.014958\nlocation_le      0.011508\nfac_class       -0.000067\npro_class       -0.016752\nsmile           -0.024376\nrot_class       -0.034287\ndet_class       -0.044508\ndelta2          -0.044728\nPCA3            -0.047842\ndob_class       -0.049204\nvb              -0.066128\nnsu_class       -0.079306\nPCA1            -0.082138\nfirst_words_p   -0.085390\nneg_tweet       -0.086476\npos_tweet       -0.098833\nadv_class       -0.114662\ndelta           -0.162080\npositive        -0.173409\npositive%       -0.173843\nsad                   NaN\ninj                   NaN\nName: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "matrix_corr = df_train.corr(method ='spearman')\n",
    "matrix_corr = matrix_corr['target'].sort_values(ascending=False)\n",
    "print(matrix_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "\n",
    "scaler_atr = StandardScaler()\n",
    "\n",
    "features = df_train[['loc_class','car_class','nor_class','amd_class','adv_class','neg_tweet',\n",
    "                    'negative','pos_tweet','mf_class_le']]\n",
    "\n",
    "features1 = df_test[['loc_class','car_class','nor_class','amd_class','adv_class','neg_tweet',\n",
    "                    'negative','pos_tweet','mf_class_le']]\n",
    "\n",
    "\n",
    "atb = scaler_atr.fit_transform(features)\n",
    "X = np.matrix(atb)\n",
    "S = np.cov(np.transpose(X))\n",
    "\n",
    "atb1 = scaler_atr.fit_transform(features1)\n",
    "X1 = np.matrix(atb1)\n",
    "S1 = np.cov(np.transpose(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca1 = PCA(n_components=4)\n",
    "\n",
    "\n",
    "pca.fit(X)\n",
    "pca1.fit(X1)\n",
    "\n",
    "\n",
    "components = np.round(pca.explained_variance_ratio_ , 2)\n",
    "components1 = np.round(pca1.explained_variance_ratio_ , 2)\n",
    "\n",
    "pca_1 = pca.transform(X)[:,0]\n",
    "pca_2 = pca.transform(X)[:,1]\n",
    "pca_3 = pca.transform(X)[:,2]\n",
    "pca_4 = pca.transform(X)[:,3]\n",
    "\n",
    "df_train['PCA1'] = pca_1\n",
    "df_train['PCA2'] = pca_2\n",
    "df_train['PCA3'] = pca_3\n",
    "df_train['PCA4'] = pca_4\n",
    "\n",
    "pca1_1 = pca1.transform(X1)[:,0]\n",
    "pca1_2 = pca1.transform(X1)[:,1]\n",
    "pca1_3 = pca1.transform(X1)[:,2]\n",
    "pca1_4 = pca1.transform(X1)[:,3]\n",
    "\n",
    "df_test['PCA1'] = pca1_1\n",
    "df_test['PCA2'] = pca1_2\n",
    "df_test['PCA3'] = pca1_3\n",
    "df_test['PCA4'] = pca1_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target           1.000000\ndis_words        0.367625\nPCA2             0.212402\npob_class        0.188962\npre_class        0.182836\nsubs             0.177987\nmf_class_le      0.173360\nemoji_n          0.168089\ncom_class        0.168062\ngpe_class        0.125694\namd_class        0.124252\ncar_class        0.118271\nnegative         0.084004\nnegative%        0.073709\nfirst_words_n    0.072950\nadj              0.070448\nqtd_class        0.067198\norg_class        0.064752\nid               0.060789\nnor_class        0.050498\ndat_class        0.048408\nkeyword_le       0.047864\nneutro           0.044140\nwords_number     0.041779\ntim_class        0.040029\nmon_class        0.030994\nper_class        0.026778\nsp_class_le      0.021905\nloc_class        0.019182\npun_class        0.015417\nPCA4             0.014958\nlocation_le      0.011508\nfac_class       -0.000067\npro_class       -0.016752\nsmile           -0.024376\nrot_class       -0.034287\ndet_class       -0.044508\ndelta2          -0.044728\nPCA3            -0.047842\ndob_class       -0.049204\nvb              -0.066128\nnsu_class       -0.079306\nPCA1            -0.082138\nfirst_words_p   -0.085390\nneg_tweet       -0.086476\npos_tweet       -0.098833\nadv_class       -0.114662\ndelta           -0.162080\npositive        -0.173409\npositive%       -0.173843\nsad                   NaN\ninj                   NaN\nName: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "matrix_corr = df_train.corr(method ='spearman')\n",
    "matrix_corr = matrix_corr['target'].sort_values(ascending=False)\n",
    "print(matrix_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best features to predict Response :Customer_Freq , AcceptedCmp5 ,AcceptedCmp1\n",
    "# spliting features and label\n",
    "\n",
    "train_features = df_train[['dis_words','gpe_class','pob_class','com_class','subs',\n",
    "                           'pre_class','emoji_n','positive','delta','PCA2']]\n",
    "train_labels = df_train['target']\n",
    "\n",
    "test_features = df_test[['dis_words','gpe_class','pob_class','com_class','subs',\n",
    "                           'pre_class','emoji_n','positive','delta','PCA2']]\n",
    "test_labels = df_sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks input params\n",
    "\n",
    "units = 100 # number of neurons in first layer\n",
    "activation = 'relu' # activation function\n",
    "kernel_initializer = 'random_uniform' # initialize weights\n",
    "input_dim = 10    # input layer - number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating keras classifier neural networks Dense (Full connected)\n",
    "\n",
    "clf = Sequential()\n",
    "# input layer\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ,\n",
    "              input_dim = input_dim ))\n",
    "# hidden layer 1\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 2\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 3\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 4\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 5\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 6\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 7\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "# hidden layer 8\n",
    "clf.add(Dense(units = units, activation = activation , kernel_initializer = kernel_initializer ))\n",
    "\n",
    "# neural networks output params\n",
    "units1 = 1 # binary output decision\n",
    "activation1 = 'sigmoid' # activation function of binary classification\n",
    "\n",
    "# output layer\n",
    "clf.add(Dense(units = units1 , activation = activation1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer model params \n",
    "\n",
    "optmizer = 'adam' # function of weights adjust\n",
    "loss = 'binary_crossentropy' # loss function, error evaluate\n",
    "metrics = 'binary_accuracy' # evaluate metric\n",
    "clf.compile(optimizer = optmizer , loss = loss , metrics = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===============] - 6s 843us/step - loss: 0.4345 - binary_accuracy: 0.8020\n",
      "Epoch 76/250\n",
      "7613/7613 [==============================] - 6s 849us/step - loss: 0.4526 - binary_accuracy: 0.7876\n",
      "Epoch 77/250\n",
      "7613/7613 [==============================] - 6s 838us/step - loss: 0.4330 - binary_accuracy: 0.7971\n",
      "Epoch 78/250\n",
      "7613/7613 [==============================] - 7s 859us/step - loss: 0.4453 - binary_accuracy: 0.7988\n",
      "Epoch 79/250\n",
      "7613/7613 [==============================] - 6s 837us/step - loss: 0.4291 - binary_accuracy: 0.8114\n",
      "Epoch 80/250\n",
      "7613/7613 [==============================] - 7s 861us/step - loss: 0.4336 - binary_accuracy: 0.8035\n",
      "Epoch 81/250\n",
      "7613/7613 [==============================] - 6s 837us/step - loss: 0.4233 - binary_accuracy: 0.8073\n",
      "Epoch 82/250\n",
      "7613/7613 [==============================] - 6s 810us/step - loss: 0.4405 - binary_accuracy: 0.7942\n",
      "Epoch 83/250\n",
      "7613/7613 [==============================] - 6s 815us/step - loss: 0.4470 - binary_accuracy: 0.7935\n",
      "Epoch 84/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.4180 - binary_accuracy: 0.8156\n",
      "Epoch 85/250\n",
      "7613/7613 [==============================] - 6s 818us/step - loss: 0.4235 - binary_accuracy: 0.8079\n",
      "Epoch 86/250\n",
      "7613/7613 [==============================] - 7s 857us/step - loss: 0.4240 - binary_accuracy: 0.8116\n",
      "Epoch 87/250\n",
      "7613/7613 [==============================] - 6s 821us/step - loss: 0.4233 - binary_accuracy: 0.7989\n",
      "Epoch 88/250\n",
      "7613/7613 [==============================] - 6s 815us/step - loss: 0.4195 - binary_accuracy: 0.8082\n",
      "Epoch 89/250\n",
      "7613/7613 [==============================] - 6s 785us/step - loss: 0.4134 - binary_accuracy: 0.8122\n",
      "Epoch 90/250\n",
      "7613/7613 [==============================] - 6s 782us/step - loss: 0.4723 - binary_accuracy: 0.8038\n",
      "Epoch 91/250\n",
      "7613/7613 [==============================] - 6s 786us/step - loss: 0.4534 - binary_accuracy: 0.8026\n",
      "Epoch 92/250\n",
      "7613/7613 [==============================] - 6s 780us/step - loss: 0.4352 - binary_accuracy: 0.8105\n",
      "Epoch 93/250\n",
      "7613/7613 [==============================] - 6s 787us/step - loss: 0.4265 - binary_accuracy: 0.8050\n",
      "Epoch 94/250\n",
      "7613/7613 [==============================] - 6s 773us/step - loss: 0.4246 - binary_accuracy: 0.8070\n",
      "Epoch 95/250\n",
      "7613/7613 [==============================] - 6s 781us/step - loss: 0.4239 - binary_accuracy: 0.8088\n",
      "Epoch 96/250\n",
      "7613/7613 [==============================] - 6s 815us/step - loss: 0.4260 - binary_accuracy: 0.8061\n",
      "Epoch 97/250\n",
      "7613/7613 [==============================] - 6s 800us/step - loss: 0.4262 - binary_accuracy: 0.8127\n",
      "Epoch 98/250\n",
      "7613/7613 [==============================] - 6s 785us/step - loss: 0.4211 - binary_accuracy: 0.8122\n",
      "Epoch 99/250\n",
      "7613/7613 [==============================] - 6s 788us/step - loss: 0.4105 - binary_accuracy: 0.8091\n",
      "Epoch 100/250\n",
      "7613/7613 [==============================] - 6s 779us/step - loss: 0.4192 - binary_accuracy: 0.8174\n",
      "Epoch 101/250\n",
      "7613/7613 [==============================] - 6s 782us/step - loss: 0.4267 - binary_accuracy: 0.8058\n",
      "Epoch 102/250\n",
      "7613/7613 [==============================] - 6s 777us/step - loss: 0.4001 - binary_accuracy: 0.8194\n",
      "Epoch 103/250\n",
      "7613/7613 [==============================] - 6s 784us/step - loss: 0.4139 - binary_accuracy: 0.8169\n",
      "Epoch 104/250\n",
      "7613/7613 [==============================] - 6s 777us/step - loss: 0.4130 - binary_accuracy: 0.8172\n",
      "Epoch 105/250\n",
      "7613/7613 [==============================] - 6s 787us/step - loss: 0.4177 - binary_accuracy: 0.8092\n",
      "Epoch 106/250\n",
      "7613/7613 [==============================] - 6s 786us/step - loss: 0.4260 - binary_accuracy: 0.8060\n",
      "Epoch 107/250\n",
      "7613/7613 [==============================] - 6s 851us/step - loss: 0.4496 - binary_accuracy: 0.8063\n",
      "Epoch 108/250\n",
      "7613/7613 [==============================] - 7s 863us/step - loss: 0.4241 - binary_accuracy: 0.8091\n",
      "Epoch 109/250\n",
      "7613/7613 [==============================] - 6s 821us/step - loss: 0.4098 - binary_accuracy: 0.8140\n",
      "Epoch 110/250\n",
      "7613/7613 [==============================] - 6s 789us/step - loss: 0.4226 - binary_accuracy: 0.8129\n",
      "Epoch 111/250\n",
      "7613/7613 [==============================] - 6s 779us/step - loss: 0.4168 - binary_accuracy: 0.8093\n",
      "Epoch 112/250\n",
      "7613/7613 [==============================] - 6s 781us/step - loss: 0.4093 - binary_accuracy: 0.8158\n",
      "Epoch 113/250\n",
      "7613/7613 [==============================] - 6s 793us/step - loss: 0.4327 - binary_accuracy: 0.8041\n",
      "Epoch 114/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.4123 - binary_accuracy: 0.8116\n",
      "Epoch 115/250\n",
      "7613/7613 [==============================] - 6s 839us/step - loss: 0.4458 - binary_accuracy: 0.7981\n",
      "Epoch 116/250\n",
      "7613/7613 [==============================] - 6s 786us/step - loss: 0.4137 - binary_accuracy: 0.8120\n",
      "Epoch 117/250\n",
      "7613/7613 [==============================] - 6s 782us/step - loss: 0.4081 - binary_accuracy: 0.8131\n",
      "Epoch 118/250\n",
      "7613/7613 [==============================] - 6s 798us/step - loss: 0.4053 - binary_accuracy: 0.8191\n",
      "Epoch 119/250\n",
      "7613/7613 [==============================] - 6s 797us/step - loss: 0.4180 - binary_accuracy: 0.8145\n",
      "Epoch 120/250\n",
      "7613/7613 [==============================] - 6s 780us/step - loss: 0.4267 - binary_accuracy: 0.8143\n",
      "Epoch 121/250\n",
      "7613/7613 [==============================] - 6s 785us/step - loss: 0.4103 - binary_accuracy: 0.8125\n",
      "Epoch 122/250\n",
      "7613/7613 [==============================] - 6s 781us/step - loss: 0.3990 - binary_accuracy: 0.8129\n",
      "Epoch 123/250\n",
      "7613/7613 [==============================] - 6s 778us/step - loss: 0.4298 - binary_accuracy: 0.8097\n",
      "Epoch 124/250\n",
      "7613/7613 [==============================] - 6s 787us/step - loss: 0.4020 - binary_accuracy: 0.8156\n",
      "Epoch 125/250\n",
      "7613/7613 [==============================] - 6s 787us/step - loss: 0.4164 - binary_accuracy: 0.8156\n",
      "Epoch 126/250\n",
      "7613/7613 [==============================] - 6s 778us/step - loss: 0.4040 - binary_accuracy: 0.8141\n",
      "Epoch 127/250\n",
      "7613/7613 [==============================] - 6s 784us/step - loss: 0.4052 - binary_accuracy: 0.8209\n",
      "Epoch 128/250\n",
      "7613/7613 [==============================] - 6s 812us/step - loss: 0.3936 - binary_accuracy: 0.8214\n",
      "Epoch 129/250\n",
      "7613/7613 [==============================] - 6s 852us/step - loss: 0.3956 - binary_accuracy: 0.8208\n",
      "Epoch 130/250\n",
      "7613/7613 [==============================] - 6s 819us/step - loss: 0.4142 - binary_accuracy: 0.8115\n",
      "Epoch 131/250\n",
      "7613/7613 [==============================] - 6s 816us/step - loss: 0.4486 - binary_accuracy: 0.8140\n",
      "Epoch 132/250\n",
      "7613/7613 [==============================] - 6s 806us/step - loss: 0.4106 - binary_accuracy: 0.8144\n",
      "Epoch 133/250\n",
      "7613/7613 [==============================] - 6s 808us/step - loss: 0.4127 - binary_accuracy: 0.8236\n",
      "Epoch 134/250\n",
      "7613/7613 [==============================] - 6s 803us/step - loss: 0.4151 - binary_accuracy: 0.8210\n",
      "Epoch 135/250\n",
      "7613/7613 [==============================] - 6s 807us/step - loss: 0.4119 - binary_accuracy: 0.8262\n",
      "Epoch 136/250\n",
      "7613/7613 [==============================] - 6s 803us/step - loss: 0.4392 - binary_accuracy: 0.8185\n",
      "Epoch 137/250\n",
      "7613/7613 [==============================] - 6s 804us/step - loss: 0.5686 - binary_accuracy: 0.8255\n",
      "Epoch 138/250\n",
      "7613/7613 [==============================] - 6s 804us/step - loss: 0.4073 - binary_accuracy: 0.8230\n",
      "Epoch 139/250\n",
      "7613/7613 [==============================] - 6s 814us/step - loss: 0.4202 - binary_accuracy: 0.8212\n",
      "Epoch 140/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.4399 - binary_accuracy: 0.8033\n",
      "Epoch 141/250\n",
      "7613/7613 [==============================] - 6s 815us/step - loss: 0.4267 - binary_accuracy: 0.8142\n",
      "Epoch 142/250\n",
      "7613/7613 [==============================] - 6s 797us/step - loss: 0.4335 - binary_accuracy: 0.8062\n",
      "Epoch 143/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4368 - binary_accuracy: 0.8075\n",
      "Epoch 144/250\n",
      "7613/7613 [==============================] - 6s 802us/step - loss: 0.4678 - binary_accuracy: 0.8134\n",
      "Epoch 145/250\n",
      "7613/7613 [==============================] - 6s 806us/step - loss: 0.4503 - binary_accuracy: 0.8082\n",
      "Epoch 146/250\n",
      "7613/7613 [==============================] - 6s 821us/step - loss: 0.4486 - binary_accuracy: 0.8214\n",
      "Epoch 147/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4061 - binary_accuracy: 0.8164\n",
      "Epoch 148/250\n",
      "7613/7613 [==============================] - 6s 806us/step - loss: 0.4003 - binary_accuracy: 0.8273\n",
      "Epoch 149/250\n",
      "7613/7613 [==============================] - 6s 811us/step - loss: 0.4131 - binary_accuracy: 0.8174\n",
      "Epoch 150/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4208 - binary_accuracy: 0.7976\n",
      "Epoch 151/250\n",
      "7613/7613 [==============================] - 6s 829us/step - loss: 0.4475 - binary_accuracy: 0.8043\n",
      "Epoch 152/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.3998 - binary_accuracy: 0.8184\n",
      "Epoch 153/250\n",
      "7613/7613 [==============================] - 6s 800us/step - loss: 0.4730 - binary_accuracy: 0.8068\n",
      "Epoch 154/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4237 - binary_accuracy: 0.7994\n",
      "Epoch 155/250\n",
      "7613/7613 [==============================] - 6s 802us/step - loss: 0.4442 - binary_accuracy: 0.8039\n",
      "Epoch 156/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4202 - binary_accuracy: 0.8035\n",
      "Epoch 157/250\n",
      "7613/7613 [==============================] - 6s 831us/step - loss: 0.4169 - binary_accuracy: 0.8146\n",
      "Epoch 158/250\n",
      "7613/7613 [==============================] - 6s 814us/step - loss: 0.4104 - binary_accuracy: 0.8198\n",
      "Epoch 159/250\n",
      "7613/7613 [==============================] - 6s 807us/step - loss: 0.4023 - binary_accuracy: 0.8169\n",
      "Epoch 160/250\n",
      "7613/7613 [==============================] - 6s 807us/step - loss: 0.4173 - binary_accuracy: 0.8114\n",
      "Epoch 161/250\n",
      "7613/7613 [==============================] - 6s 824us/step - loss: 0.4601 - binary_accuracy: 0.8067\n",
      "Epoch 162/250\n",
      "7613/7613 [==============================] - 6s 813us/step - loss: 0.4459 - binary_accuracy: 0.8076\n",
      "Epoch 163/250\n",
      "7613/7613 [==============================] - 6s 814us/step - loss: 0.4241 - binary_accuracy: 0.8131\n",
      "Epoch 164/250\n",
      "7613/7613 [==============================] - 6s 811us/step - loss: 0.4341 - binary_accuracy: 0.8119\n",
      "Epoch 165/250\n",
      "7613/7613 [==============================] - 6s 800us/step - loss: 0.4439 - binary_accuracy: 0.8094\n",
      "Epoch 166/250\n",
      "7613/7613 [==============================] - 6s 807us/step - loss: 0.4255 - binary_accuracy: 0.8128\n",
      "Epoch 167/250\n",
      "7613/7613 [==============================] - 6s 815us/step - loss: 0.4189 - binary_accuracy: 0.8151\n",
      "Epoch 168/250\n",
      "7613/7613 [==============================] - 6s 812us/step - loss: 0.4573 - binary_accuracy: 0.8181\n",
      "Epoch 169/250\n",
      "7613/7613 [==============================] - 6s 800us/step - loss: 0.4339 - binary_accuracy: 0.8079\n",
      "Epoch 170/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4462 - binary_accuracy: 0.8168\n",
      "Epoch 171/250\n",
      "7613/7613 [==============================] - 6s 813us/step - loss: 0.4267 - binary_accuracy: 0.8117\n",
      "Epoch 172/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.4300 - binary_accuracy: 0.8203\n",
      "Epoch 173/250\n",
      "7613/7613 [==============================] - 6s 809us/step - loss: 0.4046 - binary_accuracy: 0.8190\n",
      "Epoch 174/250\n",
      "7613/7613 [==============================] - 6s 812us/step - loss: 0.4077 - binary_accuracy: 0.8232\n",
      "Epoch 175/250\n",
      "7613/7613 [==============================] - 6s 812us/step - loss: 0.4169 - binary_accuracy: 0.8208\n",
      "Epoch 176/250\n",
      "7613/7613 [==============================] - 6s 806us/step - loss: 0.4482 - binary_accuracy: 0.8221\n",
      "Epoch 177/250\n",
      "7613/7613 [==============================] - 6s 807us/step - loss: 0.5075 - binary_accuracy: 0.8117\n",
      "Epoch 178/250\n",
      "7613/7613 [==============================] - 6s 811us/step - loss: 0.4610 - binary_accuracy: 0.8212\n",
      "Epoch 179/250\n",
      "7613/7613 [==============================] - 6s 812us/step - loss: 0.3943 - binary_accuracy: 0.8359\n",
      "Epoch 180/250\n",
      "7613/7613 [==============================] - 7s 898us/step - loss: 0.4071 - binary_accuracy: 0.8295\n",
      "Epoch 181/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.4235 - binary_accuracy: 0.8323\n",
      "Epoch 182/250\n",
      "7613/7613 [==============================] - 6s 843us/step - loss: 0.4367 - binary_accuracy: 0.8281\n",
      "Epoch 183/250\n",
      "7613/7613 [==============================] - 7s 854us/step - loss: 0.4414 - binary_accuracy: 0.8386\n",
      "Epoch 184/250\n",
      "7613/7613 [==============================] - 6s 825us/step - loss: 0.4023 - binary_accuracy: 0.8291\n",
      "Epoch 185/250\n",
      "7613/7613 [==============================] - 6s 832us/step - loss: 0.4446 - binary_accuracy: 0.8297\n",
      "Epoch 186/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.4167 - binary_accuracy: 0.8350\n",
      "Epoch 187/250\n",
      "7613/7613 [==============================] - 6s 831us/step - loss: 0.4184 - binary_accuracy: 0.8349\n",
      "Epoch 188/250\n",
      "7613/7613 [==============================] - 6s 828us/step - loss: 0.4231 - binary_accuracy: 0.8247\n",
      "Epoch 189/250\n",
      "7613/7613 [==============================] - 6s 824us/step - loss: 0.4281 - binary_accuracy: 0.8285\n",
      "Epoch 190/250\n",
      "7613/7613 [==============================] - 6s 834us/step - loss: 0.4580 - binary_accuracy: 0.8161\n",
      "Epoch 191/250\n",
      "7613/7613 [==============================] - 6s 828us/step - loss: 0.4224 - binary_accuracy: 0.8307\n",
      "Epoch 192/250\n",
      "7613/7613 [==============================] - 6s 834us/step - loss: 0.5016 - binary_accuracy: 0.8166\n",
      "Epoch 193/250\n",
      "7613/7613 [==============================] - 7s 866us/step - loss: 0.4928 - binary_accuracy: 0.8078\n",
      "Epoch 194/250\n",
      "7613/7613 [==============================] - 7s 854us/step - loss: 0.5828 - binary_accuracy: 0.7834\n",
      "Epoch 195/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.5214 - binary_accuracy: 0.7770\n",
      "Epoch 196/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.5548 - binary_accuracy: 0.7694\n",
      "Epoch 197/250\n",
      "7613/7613 [==============================] - 6s 831us/step - loss: 0.5459 - binary_accuracy: 0.7805\n",
      "Epoch 198/250\n",
      "7613/7613 [==============================] - 6s 842us/step - loss: 0.4984 - binary_accuracy: 0.7997\n",
      "Epoch 199/250\n",
      "7613/7613 [==============================] - 6s 829us/step - loss: 0.5171 - binary_accuracy: 0.7964\n",
      "Epoch 200/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.5590 - binary_accuracy: 0.7835\n",
      "Epoch 201/250\n",
      "7613/7613 [==============================] - 6s 829us/step - loss: 0.5974 - binary_accuracy: 0.7497\n",
      "Epoch 202/250\n",
      "7613/7613 [==============================] - 6s 828us/step - loss: 0.5971 - binary_accuracy: 0.7290\n",
      "Epoch 203/250\n",
      "7613/7613 [==============================] - 7s 866us/step - loss: 0.6031 - binary_accuracy: 0.7164\n",
      "Epoch 204/250\n",
      "7613/7613 [==============================] - 7s 882us/step - loss: 0.5925 - binary_accuracy: 0.7195\n",
      "Epoch 205/250\n",
      "7613/7613 [==============================] - 7s 877us/step - loss: 0.6192 - binary_accuracy: 0.6869\n",
      "Epoch 206/250\n",
      "7613/7613 [==============================] - 7s 862us/step - loss: 0.5706 - binary_accuracy: 0.7173\n",
      "Epoch 207/250\n",
      "7613/7613 [==============================] - 6s 847us/step - loss: 0.5708 - binary_accuracy: 0.7305\n",
      "Epoch 208/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.5396 - binary_accuracy: 0.7475\n",
      "Epoch 209/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.5666 - binary_accuracy: 0.7206\n",
      "Epoch 210/250\n",
      "7613/7613 [==============================] - 7s 869us/step - loss: 0.5309 - binary_accuracy: 0.7700\n",
      "Epoch 211/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.5278 - binary_accuracy: 0.7760\n",
      "Epoch 212/250\n",
      "7613/7613 [==============================] - 6s 832us/step - loss: 0.5302 - binary_accuracy: 0.7890\n",
      "Epoch 213/250\n",
      "7613/7613 [==============================] - 6s 852us/step - loss: 0.5382 - binary_accuracy: 0.7707\n",
      "Epoch 214/250\n",
      "7613/7613 [==============================] - 6s 841us/step - loss: 0.5686 - binary_accuracy: 0.7568\n",
      "Epoch 215/250\n",
      "7613/7613 [==============================] - 6s 834us/step - loss: 0.4991 - binary_accuracy: 0.7959\n",
      "Epoch 216/250\n",
      "7613/7613 [==============================] - 6s 835us/step - loss: 0.5359 - binary_accuracy: 0.7763\n",
      "Epoch 217/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.5621 - binary_accuracy: 0.7562\n",
      "Epoch 218/250\n",
      "7613/7613 [==============================] - 7s 865us/step - loss: 0.5096 - binary_accuracy: 0.7836\n",
      "Epoch 219/250\n",
      "7613/7613 [==============================] - 6s 828us/step - loss: 0.5083 - binary_accuracy: 0.7917\n",
      "Epoch 220/250\n",
      "7613/7613 [==============================] - 6s 833us/step - loss: 0.5089 - binary_accuracy: 0.7920\n",
      "Epoch 221/250\n",
      "7613/7613 [==============================] - 6s 833us/step - loss: 0.5120 - binary_accuracy: 0.7835\n",
      "Epoch 222/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.5160 - binary_accuracy: 0.7868\n",
      "Epoch 223/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.5263 - binary_accuracy: 0.7821\n",
      "Epoch 224/250\n",
      "7613/7613 [==============================] - 7s 866us/step - loss: 0.4875 - binary_accuracy: 0.8018\n",
      "Epoch 225/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.4800 - binary_accuracy: 0.8073\n",
      "Epoch 226/250\n",
      "7613/7613 [==============================] - 7s 890us/step - loss: 0.4818 - binary_accuracy: 0.8007\n",
      "Epoch 227/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.4630 - binary_accuracy: 0.8185\n",
      "Epoch 228/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.4960 - binary_accuracy: 0.8006\n",
      "Epoch 229/250\n",
      "7613/7613 [==============================] - 6s 823us/step - loss: 0.4819 - binary_accuracy: 0.8060\n",
      "Epoch 230/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.5159 - binary_accuracy: 0.7851\n",
      "Epoch 231/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.4905 - binary_accuracy: 0.8004\n",
      "Epoch 232/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.4906 - binary_accuracy: 0.8018\n",
      "Epoch 233/250\n",
      "7613/7613 [==============================] - 6s 823us/step - loss: 0.5527 - binary_accuracy: 0.7926\n",
      "Epoch 234/250\n",
      "7613/7613 [==============================] - 6s 850us/step - loss: 0.4978 - binary_accuracy: 0.7964\n",
      "Epoch 235/250\n",
      "7613/7613 [==============================] - 6s 825us/step - loss: 0.5369 - binary_accuracy: 0.7704\n",
      "Epoch 236/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.5468 - binary_accuracy: 0.7492\n",
      "Epoch 237/250\n",
      "7613/7613 [==============================] - 6s 826us/step - loss: 0.5368 - binary_accuracy: 0.7763\n",
      "Epoch 238/250\n",
      "7613/7613 [==============================] - 6s 823us/step - loss: 0.6579 - binary_accuracy: 0.7486\n",
      "Epoch 239/250\n",
      "7613/7613 [==============================] - 6s 817us/step - loss: 0.5331 - binary_accuracy: 0.7738\n",
      "Epoch 240/250\n",
      "7613/7613 [==============================] - 6s 816us/step - loss: 0.5416 - binary_accuracy: 0.7635\n",
      "Epoch 241/250\n",
      "7613/7613 [==============================] - 6s 830us/step - loss: 0.5783 - binary_accuracy: 0.7207\n",
      "Epoch 242/250\n",
      "7613/7613 [==============================] - 6s 821us/step - loss: 0.5686 - binary_accuracy: 0.7501\n",
      "Epoch 243/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.5712 - binary_accuracy: 0.7372\n",
      "Epoch 244/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.5580 - binary_accuracy: 0.7489\n",
      "Epoch 245/250\n",
      "7613/7613 [==============================] - 6s 851us/step - loss: 0.6054 - binary_accuracy: 0.6846\n",
      "Epoch 246/250\n",
      "7613/7613 [==============================] - 6s 820us/step - loss: 0.6204 - binary_accuracy: 0.7370\n",
      "Epoch 247/250\n",
      "7613/7613 [==============================] - 6s 824us/step - loss: 0.5722 - binary_accuracy: 0.7413\n",
      "Epoch 248/250\n",
      "7613/7613 [==============================] - 6s 819us/step - loss: 0.5561 - binary_accuracy: 0.7460\n",
      "Epoch 249/250\n",
      "7613/7613 [==============================] - 6s 825us/step - loss: 0.5606 - binary_accuracy: 0.7500\n",
      "Epoch 250/250\n",
      "7613/7613 [==============================] - 6s 827us/step - loss: 0.5623 - binary_accuracy: 0.7470\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2021d14ac10>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "clf.fit(train_features , train_labels , batch_size = 1 , epochs = 150 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "\n",
    "predictions = clf.predict(test_features)\n",
    "predictions = (predictions > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Keras Neural Network Accuracy:  73.76647257125344\nKeras F1 Score:  0.4245149911816578\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "\n",
    "acc = sklearn.metrics.accuracy_score(test_labels , predictions)\n",
    "print('Keras Neural Network Accuracy: ', acc*100)\n",
    "\n",
    "# f1\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_labels , predictions,average='macro')\n",
    "print('Keras F1 Score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}